# Twitter Threads

## Mine

* [Multiplying Matrices Without Multiplying](https://twitter.com/davisblalock/status/1407384407808221187)
* [Generating Training Data with Language Models: Towards Zero-Shot Language Understanding](https://twitter.com/davisblalock/status/1493972185282662402)
* [Universal Hopfield Networks: A General Framework for Single-Shot Associative Memory Models](https://twitter.com/davisblalock/status/1493606777602015232)
* [Understanding Rare Spurious Correlations in Neural Networks](https://twitter.com/davisblalock/status/1493239357028782080)
* [Locating and Editing Factual Knowledge in GPT](https://twitter.com/davisblalock/status/1492910702415122432)
* [Building a Performance Model for Deep Learning Recommendation Model Training on GPUs](https://twitter.com/davisblalock/status/1485501438352523264)
* [The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks](https://twitter.com/davisblalock/status/1485501439954669569)
* [Improving language models by retrieving from trillions of tokens](https://twitter.com/davisblalock/status/1478634719885496320)
* [Editing a classifier by rewriting its prediction rules](https://twitter.com/davisblalock/status/1470309981920497667)
* [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://twitter.com/davisblalock/status/1470303000220426241)
* [Frustratingly Simple Pretraining Alternatives to Masked Language Modeling](https://twitter.com/davisblalock/status/1439958273940238337)
* [C-MinHash: Rigorously Reducing K Permutations to Two](https://twitter.com/davisblalock/status/1439594124332568585)
* [edge-SR: Super-Resolution For The Masses](https://twitter.com/davisblalock/status/1431871339317501956)
*

## Others

* [Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning](https://twitter.com/utkuevci/status/1481005647378456578)
* [Deduplicating Training Data Makes Language Models Better](https://twitter.com/katherine1ee/status/1415496898241339400)
